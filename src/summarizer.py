# src/summarizer.py
import json
import concurrent.futures
from typing import List, Dict
from openai import OpenAI
from .config import OPENAI_API_KEY, OPENAI_BASE_URL, LLM_MODEL, TOKEN_SAVING_MODE

from .preferences import USER_INTERESTS, USER_DISLIKES
from .memory_manager import MemoryManager

class NewsSummarizer:
    def __init__(self):
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not found in environment variables.")
        
        self.client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        self.model = LLM_MODEL
        self.memory = MemoryManager()

    def batch_filter_articles(self, news_items: List[Dict]) -> List[Dict]:
        """
        [Filter Step] Use a single cheap LLM call to filter out irrelevant news by title.
        """
        if not news_items:
            return []
            
        print(f"ğŸ” [Token Saving] Batch filtering {len(news_items)} articles by title...")
        
        # Prepare list for prompt
        titles_text = ""
        for i, item in enumerate(news_items):
            titles_text += f"{i}. {item['title']} (Source: {item['source']})\n"
            
        prompt = f"""
è¯·ä½œä¸ºä¸€åä¸¥æ ¼çš„ AI æ–°é—»ç¼–è¾‘ï¼Œä»ä»¥ä¸‹åˆ—è¡¨ä¸­ç­›é€‰å‡º**çœŸæ­£é‡è¦**ä¸”**ç¬¦åˆç”¨æˆ·å…´è¶£**çš„æ–°é—»ã€‚

ç”¨æˆ·å…´è¶£: {", ".join(USER_INTERESTS)}
ä¸æ„Ÿå…´è¶£: {", ".join(USER_DISLIKES)}

ç­›é€‰æ ‡å‡†ï¼š
1. å¿…é¡»æ˜¯ AI é¢†åŸŸçš„**é‡å¤§**è¿›å±•ã€æ–°æ¨¡å‹å‘å¸ƒã€é‡è¦ç ”ç©¶æˆ–å•†ä¸šå¤§äº‹ä»¶ã€‚
2. å‰”é™¤ï¼šæ•™ç¨‹ç±»("How to")ã€è¿‡äºç»†åˆ†çš„æ¯æ—¥è®ºæ–‡ã€æ— å…³çš„æ¨å¹¿ã€é‡å¤çš„æŠ¥é“ã€‚
3. ä¸¥æ ¼æ§åˆ¶æ•°é‡ï¼Œåªä¿ç•™æœ€æœ‰ä»·å€¼çš„å‰ 30%-50%ã€‚

æ–°é—»åˆ—è¡¨ï¼š
{titles_text}

è¯·ä»…è¾“å‡ºä¿ç•™çš„æ–°é—»ç¼–å·åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ JSONï¼š
{{
    "keep_indices": [0, 2, 5, ...]
}}
"""
        try:
            response = self.client.chat.completions.create(
                model=self.model, # Can use a cheaper model here if available
                messages=[
                    {"role": "system", "content": "You are a strict news editor. Output JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            result = json.loads(response.choices[0].message.content)
            keep_indices = set(result.get("keep_indices", []))
            
            filtered_items = [item for i, item in enumerate(news_items) if i in keep_indices]
            print(f"ğŸ“‰ Filtered down to {len(filtered_items)} items (from {len(news_items)})")
            return filtered_items
            
        except Exception as e:
            print(f"âš ï¸ Filter failed, keeping all items: {e}")
            return news_items

    def analyze_single_article(self, item: Dict) -> Dict:
        """
        [Map Step] Analyze a single article's full content to extract key insights.
        """
        # Truncate content if too long to save tokens
        content = item.get('full_content', item.get('summary', ''))[:3000]
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ AI æ–°é—»å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

ç”¨æˆ·åå¥½ï¼ˆè¯·æ®æ­¤è°ƒæ•´ importance_scoreï¼‰ï¼š
- é‡ç‚¹å…³æ³¨: {", ".join(USER_INTERESTS)}
- å¿½ç•¥æˆ–ä½åˆ†: {", ".join(USER_DISLIKES)}

æ ‡é¢˜ï¼š{item['title']}
æ¥æºï¼š{item['source']}
å†…å®¹ï¼š
{content}

è¯·è¾“å‡º JSON æ ¼å¼ï¼ˆä¸è¦ Markdown æ ‡è®°ï¼‰ï¼š
{{
    "title_zh": "ä¸­æ–‡æ ‡é¢˜",
    "summary_zh": "ä¸­æ–‡æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰",
    "key_points": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3"],
    "category": "æ¨¡å‹/è¡Œä¸š/å­¦æœ¯/åº”ç”¨/å…¶ä»–",
    "importance_score": 1-10 (ç¬¦åˆç”¨æˆ·å…´è¶£çš„ç»™é«˜åˆ†ï¼Œæ— å…³çš„ç»™ä½åˆ†),
    "impact_analysis": "ä¸€å¥è¯åˆ†æå…¶å¯¹è¡Œä¸šçš„å½±å“"
}}
"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an AI analyst. Output raw JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            analysis = json.loads(response.choices[0].message.content)
            
            # Merge analysis back into item
            item.update(analysis)
            return item
        except Exception as e:
            # print(f"Error analyzing {item['title']}: {e}")
            return item

    def summarize(self, news_items: List[Dict]) -> Dict:
        """
        [Reduce Step] Aggregate analyzed items into a final report.
        """
        if not news_items:
            return {}

        # 0. Pre-filtering (Token Saving)
        if TOKEN_SAVING_MODE and len(news_items) > 5:
            news_items = self.batch_filter_articles(news_items)

        # 1. Map: Parallel analysis of each article
        print(f"ğŸ§  Analyzing {len(news_items)} articles in depth...")
        analyzed_items = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(self.analyze_single_article, item) for item in news_items]
            for future in concurrent.futures.as_completed(futures):
                try:
                    analyzed_items.append(future.result())
                except Exception:
                    pass

        # 2. Filter & Sort
        # Filter out low quality items (importance < 4) or errors
        valid_items = [i for i in analyzed_items if i.get('importance_score', 0) >= 4]
        valid_items.sort(key=lambda x: x.get('importance_score', 0), reverse=True)

        # 3. Reduce: Generate final structure
        top_stories = []
        categories = {
            "æ¨¡å‹ä¸æŠ€æœ¯": [],
            "è¡Œä¸šä¸å•†ä¸š": [],
            "å­¦æœ¯ä¸ç ”ç©¶": [],
            "å·¥å…·ä¸åº”ç”¨": [],
            "å…¶ä»–": []
        }

        # Pick top 5
        for i, item in enumerate(valid_items):
            story_data = {
                "title": item.get('title_zh', item['title']),
                "summary": item.get('summary_zh', item['summary']),
                "source": item['source'],
                "link": item['link'],
                "image": item.get('image'),
                "impact": item.get('impact_analysis', ''),
                "key_points": item.get('key_points', [])
            }

            if i < 5:
                top_stories.append(story_data)
            else:
                cat = item.get('category', 'å…¶ä»–')
                # Map LLM category to our fixed keys
                target_cat = "å…¶ä»–"
                if "æ¨¡å‹" in cat or "æŠ€æœ¯" in cat: target_cat = "æ¨¡å‹ä¸æŠ€æœ¯"
                elif "è¡Œä¸š" in cat or "å•†ä¸š" in cat: target_cat = "è¡Œä¸šä¸å•†ä¸š"
                elif "å­¦æœ¯" in cat or "ç ”ç©¶" in cat: target_cat = "å­¦æœ¯ä¸ç ”ç©¶"
                elif "åº”ç”¨" in cat or "å·¥å…·" in cat: target_cat = "å·¥å…·ä¸åº”ç”¨"
                
                categories[target_cat].append(story_data)

        # Generate Intro using Top Stories + Memory Context
        history_context = self.memory.get_context_string(days=3)
        
        intro_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹ä»Šæ—¥å¤´æ¡æ–°é—»ï¼Œç»“åˆè¿‡å»å‡ å¤©çš„å†å²èƒŒæ™¯ï¼Œç”Ÿæˆä¸€å¥ç®€çŸ­çš„ä»Šæ—¥ AI è¡Œä¸šåŠ¨æ€ç»¼è¿°ã€‚

å†å²èƒŒæ™¯ï¼ˆä»…ä½œå‚è€ƒï¼Œæ— éœ€å¼ºè¡Œå…³è”ï¼‰ï¼š
{history_context}

ä»Šæ—¥å¤´æ¡ï¼š
{[t['title'] for t in top_stories]}

è¦æ±‚ï¼šç®€ç»ƒã€ä¸“ä¸šï¼Œçªå‡ºè¿ç»­æ€§ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚
"""
        try:
            intro_resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": intro_prompt}]
            )
            intro_text = intro_resp.choices[0].message.content.strip()
        except:
            intro_text = "ä»Šæ—¥ AI é¢†åŸŸæœ‰å¤šé¡¹é‡è¦æ›´æ–°ã€‚"

        final_summary = {
            "title": f"AI Daily Insight ({valid_items[0]['published'][:10] if valid_items else ''})",
            "intro": intro_text,
            "top_stories": top_stories,
            "categories": categories
        }
        
        # Save to memory
        self.memory.save_summary(final_summary)
        
        return final_summary

    def generate_deep_report(self, topic: str, research_data: list) -> str:
        """
        Generate a long-form deep dive report based on research data.
        """
        context = ""
        for i, item in enumerate(research_data):
            context += f"--- Source {i+1}: {item['title']} ---\n"
            context += f"{item['full_content'][:2000]}\n\n"

        prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹æ”¶é›†åˆ°çš„èµ„æ–™ï¼Œæ’°å†™ä¸€ä»½å…³äº "{topic}" çš„æ·±åº¦è¡Œä¸šç ”æŠ¥ã€‚

èµ„æ–™åº“ï¼š
{context}

è¦æ±‚ï¼š
1. ç»“æ„æ¸…æ™°ï¼šåŒ…å«ã€èƒŒæ™¯ä¸ç°çŠ¶ã€‘ã€ã€æ ¸å¿ƒæŠ€æœ¯/äº‹ä»¶è§£æã€‘ã€ã€å¸‚åœºç«äº‰æ ¼å±€ã€‘ã€ã€æœªæ¥è¶‹åŠ¿é¢„æµ‹ã€‘å››ä¸ªç« èŠ‚ã€‚
2. æ·±åº¦åˆ†æï¼šä¸è¦ç®€å•çš„å †ç Œèµ„æ–™ï¼Œè¦è¿›è¡Œé€»è¾‘ä¸²è”å’Œè§‚ç‚¹æç‚¼ã€‚
3. æ•°æ®æ”¯æ’‘ï¼šå¼•ç”¨èµ„æ–™ä¸­çš„å…³é”®æ•°æ®ã€‚
4. ç¯‡å¹…ï¼š1500å­—å·¦å³ã€‚
5. æ ¼å¼ï¼šMarkdownã€‚
"""
        try:
            print(f"ğŸ§  Generating deep dive report for '{topic}'...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a senior AI industry analyst."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating report: {e}"

    def generate_marp_slides(self, topic: str, report_content: str, style: str = "academic") -> str:
        """
        Convert a deep dive report into a Marp-formatted slide deck.
        Style options: 'academic' (default, strict), 'viral' (for social media/business).
        """
        
        if style == "viral":
            # Commercial/Viral Style
            theme_instruction = "theme: uncover" # A more modern/visual theme
            style_instruction = """
            3. **é£æ ¼**ï¼š
               - æå…·è§†è§‰å†²å‡»åŠ›ï¼Œé€‚åˆç¤¾äº¤åª’ä½“ä¼ æ’­ã€‚
               - æ ‡é¢˜è¦å¤¸å¼ ã€å¸å¼•çœ¼çƒï¼ˆClickbaité£æ ¼ï¼‰ã€‚
               - æ¯ä¸€é¡µå­—æ•°è¦å°‘ï¼Œé‡ç‚¹çªå‡ºé‡‘å¥ã€‚
               - **ä¸è¦**åŒ…å« Methodology é¡µã€‚
               - **ä¸è¦**å†™æ±‡æŠ¥äººåå­—ã€‚
            """
        else:
            # Default Academic Style (User Preference)
            theme_instruction = "theme: gaia"
            style_instruction = """
            3. **é£æ ¼**ï¼šä¸“ä¸šã€å­¦æœ¯ã€æç®€ã€‚
            4. **ç‰¹æ®Šè¦æ±‚**ï¼š
               - åŒ…å« "Methodology" é¡µï¼ˆç®€è¿°ç ”ç©¶æ–¹æ³•ï¼šPICOåˆ†æ/å¹¿åº¦æœç´¢+æ·±åº¦ç»¼åˆï¼‰ã€‚
               - å°é¢é¡µæ±‡æŠ¥äººå¿…é¡»å†™ï¼šåˆ˜ä½³å…´ã€‚
               - ç¦æ­¢ä½¿ç”¨å ä½ç¬¦ï¼Œå¿…é¡»æè¿°å…·ä½“çš„å›¾è¡¨å†…å®¹ã€‚
            """

        prompt = f"""
è¯·å°†ä»¥ä¸‹æ·±åº¦ç ”æŠ¥å†…å®¹è½¬æ¢ä¸º Marp (Markdown Presentation Ecosystem) æ ¼å¼çš„ PPT ä»£ç ã€‚

ç ”æŠ¥ä¸»é¢˜ï¼š{topic}
ç ”æŠ¥å†…å®¹ï¼š
{report_content[:3000]}... (æˆªå–éƒ¨åˆ†å†…å®¹)

è¦æ±‚ï¼š
1. **æ ¼å¼**ï¼šå¿…é¡»æ˜¯æ ‡å‡†çš„ Marp Markdown æ ¼å¼ã€‚
   - å¤´éƒ¨åŒ…å« `marp: true`, `{theme_instruction}`, `paginate: true`ã€‚
   - æ¯é¡µå¹»ç¯ç‰‡ç”¨ `---` åˆ†éš”ã€‚
2. **ç»“æ„**ï¼š
   - å°é¢é¡µï¼šæ ‡é¢˜ã€å‰¯æ ‡é¢˜ã€‚
   - ç›®å½•é¡µã€‚
   - æ­£æ–‡é¡µï¼šæç‚¼å…³é”®ç‚¹ï¼Œä½¿ç”¨åˆ—è¡¨ã€‚
   - ç»“æŸé¡µã€‚
{style_instruction}
5. **è¯­è¨€**ï¼šä¸­æ–‡ã€‚

è¾“å‡ºç¤ºä¾‹ï¼š
---
marp: true
{theme_instruction}
paginate: true
---

# æ ‡é¢˜
## å‰¯æ ‡é¢˜

---
...
"""
        try:
            print(f"ğŸ¨ Generating Marp slides for '{topic}' (Style: {style})...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a presentation expert skilled in Marp markdown."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5 if style == "viral" else 0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating slides: {e}"

    def generate_video_script(self, topic: str, report_content: str, platform: str = "tiktok") -> str:
        """
        Generate a short video script based on the report.
        """
        prompt = f"""
è¯·å°†ä»¥ä¸‹æ·±åº¦ç ”æŠ¥å†…å®¹æ”¹ç¼–ä¸ºä¸€ä¸ªé€‚åˆ {platform} (æŠ–éŸ³/TikTok/å°çº¢ä¹¦) çš„çŸ­è§†é¢‘å£æ’­æ–‡æ¡ˆã€‚

ä¸»é¢˜ï¼š{topic}
å‚è€ƒå†…å®¹ï¼š
{report_content[:2000]}

è¦æ±‚ï¼š
1. **é»„é‡‘å‰ä¸‰ç§’**ï¼šå¼€å¤´å¿…é¡»æœ‰ä¸€ä¸ªæå…¶æŠ“äººçš„é’©å­ï¼ˆHookï¼‰ï¼Œå¼•å‘å¥½å¥‡æˆ–ç„¦è™‘ã€‚
2. **å£è¯­åŒ–**ï¼šå®Œå…¨å¤§ç™½è¯ï¼Œä¸è¦ä¹¦é¢è¯­ï¼Œå¤šç”¨â€œå®¶äººä»¬â€ã€â€œæ³¨æ„çœ‹â€ã€â€œç»äº†â€ç­‰è¿æ¥è¯ï¼ˆè§†å¹³å°é£æ ¼è€Œå®šï¼‰ã€‚
3. **åˆ†é•œæè¿°**ï¼šå·¦ä¾§å†™ã€ç”»é¢å»ºè®®ã€‘ï¼Œå³ä¾§å†™ã€å£æ’­æ–‡æ¡ˆã€‘ã€‚
4. **æ—¶é•¿**ï¼šæ§åˆ¶åœ¨ 60-90 ç§’ï¼ˆçº¦ 200-300 å­—ï¼‰ã€‚
5. **ç»“å°¾**ï¼šå¼•å¯¼å…³æ³¨/ç‚¹èµ/è¯„è®ºã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
ã€ç”»é¢ï¼šä¸»æ’­éœ‡æƒŠè„¸ï¼ŒèƒŒæ™¯æ”¾ç›¸å…³æ–°é—»æˆªå›¾ã€‘
æ–‡æ¡ˆï¼šå¤©å‘ï¼Œè¿™ä»¶äº‹å¦‚æœçœŸçš„å‘ç”Ÿäº†ï¼Œæˆ‘ä»¬æ‰€æœ‰äººçš„é’±è¢‹å­éƒ½è¦ç¼©æ°´ï¼

ã€ç”»é¢ï¼šå±•ç¤ºæ•°æ®å›¾è¡¨ï¼Œç®­å¤´æŒ‡å‘å…³é”®ä¸‹é™è¶‹åŠ¿ã€‘
æ–‡æ¡ˆï¼šå¤§å®¶çœ‹è¿™å¼ å›¾ï¼ŒçŸ­çŸ­ä¸‰å¤©...
"""
        try:
            print(f"ğŸ¬ Generating video script for '{topic}'...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a viral content creator."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7 
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating script: {e}"
